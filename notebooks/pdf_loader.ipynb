{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ac9343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eba5713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the pdfs from the directory\n",
    "def process_all_pdfs(directory):\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(directory)\n",
    "\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    print(f'Found {len(pdf_files)} PDF files')\n",
    "    \n",
    "    for file in pdf_files:\n",
    "        print(f'\\n processing: {file.name}:')\n",
    "        try:\n",
    "            pdf_loader = PyMuPDFLoader(str(file))\n",
    "            docs = pdf_loader.load()\n",
    "\n",
    "            for doc in docs:\n",
    "                doc.metadata['source_file'] = file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "            all_documents.extend(docs)\n",
    "            print(f'loaded {len(docs)} pages')\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Error : {e}')\n",
    "    \n",
    "    print(f'Total document loaded {len(all_documents)}')\n",
    "    return all_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfb54290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 PDF files\n",
      "\n",
      " processing: encoder-decoder.pdf:\n",
      "loaded 9 pages\n",
      "\n",
      " processing: transformers_comparision.pdf:\n",
      "loaded 58 pages\n",
      "Total document loaded 67\n"
     ]
    }
   ],
   "source": [
    "all_pdf_documents = process_all_pdfs('../pdfs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58147cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create chunks\n",
    "def split_document(document,chunk_size=1000,chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = chunk_size,\n",
    "        chunk_overlap = chunk_overlap,\n",
    "        length_function = len,\n",
    "        separators = [\"\\n\\n\",\"\\n\",\" \", \"\"]\n",
    "    )\n",
    "\n",
    "    split_docs = text_splitter.split_documents(document)\n",
    "    print(f\"split {len(document)} into {len(split_docs)} chunks\")\n",
    "\n",
    "    if split_docs:\n",
    "        print(\"Example chunk\")\n",
    "        print(f\"content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5321016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split 67 into 374 chunks\n",
      "Example chunk\n",
      "content: arXiv:1409.3215v3  [cs.CL]  14 Dec 2014\n",
      "Sequence to Sequence Learning\n",
      "with Neural Networks\n",
      "Ilya Sutskever\n",
      "Google\n",
      "ilyasu@google.com\n",
      "Oriol Vinyals\n",
      "Google\n",
      "vinyals@google.com\n",
      "Quoc V. Le\n",
      "Google\n",
      "qvl@google....\n",
      "metadata: {'producer': 'GPL Ghostscript GIT PRERELEASE 9.08', 'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software', 'creationdate': '2014-12-15T20:32:57-05:00', 'source': '..\\\\pdfs\\\\encoder-decoder.pdf', 'file_path': '..\\\\pdfs\\\\encoder-decoder.pdf', 'total_pages': 9, 'format': 'PDF 1.4', 'title': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2014-12-15T20:32:57-05:00', 'trapped': '', 'modDate': \"D:20141215203257-05'00'\", 'creationDate': \"D:20141215203257-05'00'\", 'page': 0, 'source_file': 'encoder-decoder.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_document(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ee781",
   "metadata": {},
   "source": [
    "## Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc0b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84b9a16c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedding model: all-MiniLM-L6-v2\n",
      "Model successfully loaded with Embedding dimensions: 384\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    def __init__(self,model_name=\"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            print(f\"Loading Embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model successfully loaded with Embedding dimensions: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generateEmbeddings(self,text: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Genetating embedding for {len(text)} texts...\")\n",
    "        embeddings = self.model.encode(text,show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def get_embedding_dimension(self)-> int:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"model not loaded\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "\n",
    "embedding_manager = EmbeddingManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4d698a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    def __init__(self, collection_name:str = \"pdf_document\", persist_directory:str= \"../vector_store\"):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory,exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            #get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\":\"PDF document embedding for RAG\"}\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error while loading vector store: {e}\")\n",
    "\n",
    "    def add_document(self,document:list[Any], embeddings :np.ndarray):\n",
    "        if len(document) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents does not match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(document)} to the vector store...\")\n",
    "        \n",
    "        #prepare the data for chromadb\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        document_texts = []\n",
    "        embedding_lists = []\n",
    "\n",
    "        for i, (doc,embedding) in enumerate(zip(document,embeddings)):\n",
    "            #generate unique ids\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            #prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            #Document content\n",
    "            document_texts.append(doc.page_content)\n",
    "\n",
    "            #Embeddings\n",
    "            embedding_lists.append(embedding)\n",
    "\n",
    "            #Add into the collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                metadatas=metadatas,\n",
    "                embeddings=embeddings,\n",
    "                documents=document_texts\n",
    "            )\n",
    "            print(f\"Successfully add {len(document)} into the vector store\")\n",
    "            print(f\"Total documents in the collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding document into the collection: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3a73213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genetating embedding for 374 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12/12 [00:16<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (374, 384)\n",
      "Adding 374 to the vector store...\n",
      "Successfully add 374 into the vector store\n",
      "Total documents in the collection: 748\n"
     ]
    }
   ],
   "source": [
    "#convert text into embeddings\n",
    "text = [doc.page_content for doc in chunks]\n",
    "#generate embeddings\n",
    "embeddings = embedding_manager.generateEmbeddings(text)\n",
    "#store into vector store\n",
    "vectorstore.add_document(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5f5ca",
   "metadata": {},
   "source": [
    "## Retriever Pipline for Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "101ffd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    def __init__(self,vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query:str, top_k:int = 5, score_threshold:float=0.0,)->List[dict[str,any]]:\n",
    "        print(f\"Retrieving documents for query: {query}\")\n",
    "        print(f\"Top_k: {top_k}, Score_threshold: {score_threshold}\")\n",
    "        \n",
    "        #Generate embedding vector for query\n",
    "        query_embedding = self.embedding_manager.generateEmbeddings([query])[0]\n",
    "        \n",
    "        #search in the vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            #process the results\n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id,document,metadata,distance,) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    #convert distance to similarity score as chromadb uses cosine distance\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id' :doc_id,\n",
    "                            'content':document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance' : distance,\n",
    "                            'rank': 1+i\n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents afer filter\")\n",
    "            else:\n",
    "                print(\"No document found\")\n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return []\n",
    "        \n",
    "rag_retriever = RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2946fdc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query:  Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general \n",
      "Top_k: 5, Score_threshold: 0.0\n",
      "Genetating embedding for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents afer filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_c6097cfa_0',\n",
       "  'content': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014\\nSequence to Sequence Learning\\nwith Neural Networks\\nIlya Sutskever\\nGoogle\\nilyasu@google.com\\nOriol Vinyals\\nGoogle\\nvinyals@google.com\\nQuoc V. Le\\nGoogle\\nqvl@google.com\\nAbstract\\nDeep Neural Networks (DNNs) are powerful models that have achieved excel-\\nlent performance on difﬁcult learning tasks. Although DNNs work well whenever\\nlarge labeled training sets are available, they cannot be used to map sequences to\\nsequences. In this paper, we present a general end-to-end approach to sequence\\nlearning that makes minimal assumptions on the sequence structure. Our method\\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\\nto a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the\\ntarget sequence from the vector. Our main result is that on an English to French\\ntranslation task from the WMT’14 dataset, the translations produced by the LSTM\\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU',\n",
       "  'metadata': {'subject': '',\n",
       "   'trapped': '',\n",
       "   'source': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'format': 'PDF 1.4',\n",
       "   'file_path': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'page': 0,\n",
       "   'total_pages': 9,\n",
       "   'title': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014',\n",
       "   'producer': 'GPL Ghostscript GIT PRERELEASE 9.08',\n",
       "   'author': '',\n",
       "   'source_file': 'encoder-decoder.pdf',\n",
       "   'doc_index': 0,\n",
       "   'modDate': \"D:20141215203257-05'00'\",\n",
       "   'keywords': '',\n",
       "   'creationDate': \"D:20141215203257-05'00'\",\n",
       "   'creationdate': '2014-12-15T20:32:57-05:00',\n",
       "   'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software',\n",
       "   'file_type': 'pdf',\n",
       "   'content_length': 995,\n",
       "   'moddate': '2014-12-15T20:32:57-05:00'},\n",
       "  'similarity_score': 0.36680901050567627,\n",
       "  'distance': 0.6331909894943237,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_d21fee15_0',\n",
       "  'content': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014\\nSequence to Sequence Learning\\nwith Neural Networks\\nIlya Sutskever\\nGoogle\\nilyasu@google.com\\nOriol Vinyals\\nGoogle\\nvinyals@google.com\\nQuoc V. Le\\nGoogle\\nqvl@google.com\\nAbstract\\nDeep Neural Networks (DNNs) are powerful models that have achieved excel-\\nlent performance on difﬁcult learning tasks. Although DNNs work well whenever\\nlarge labeled training sets are available, they cannot be used to map sequences to\\nsequences. In this paper, we present a general end-to-end approach to sequence\\nlearning that makes minimal assumptions on the sequence structure. Our method\\nuses a multilayered Long Short-Term Memory (LSTM) to map the input sequence\\nto a vector of a ﬁxed dimensionality, and then another deep LSTM to decode the\\ntarget sequence from the vector. Our main result is that on an English to French\\ntranslation task from the WMT’14 dataset, the translations produced by the LSTM\\nachieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU',\n",
       "  'metadata': {'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software',\n",
       "   'subject': '',\n",
       "   'title': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014',\n",
       "   'keywords': '',\n",
       "   'trapped': '',\n",
       "   'page': 0,\n",
       "   'source_file': 'encoder-decoder.pdf',\n",
       "   'producer': 'GPL Ghostscript GIT PRERELEASE 9.08',\n",
       "   'doc_index': 0,\n",
       "   'format': 'PDF 1.4',\n",
       "   'moddate': '2014-12-15T20:32:57-05:00',\n",
       "   'content_length': 995,\n",
       "   'creationDate': \"D:20141215203257-05'00'\",\n",
       "   'total_pages': 9,\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2014-12-15T20:32:57-05:00',\n",
       "   'source': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'file_path': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'author': '',\n",
       "   'modDate': \"D:20141215203257-05'00'\"},\n",
       "  'similarity_score': 0.36680901050567627,\n",
       "  'distance': 0.6331909894943237,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_aa4e0afb_2',\n",
       "  'content': 'sentences (but not target sentences) improved the LSTM’s performance markedly,\\nbecause doing so introduced many short term dependencies between the source\\nand the target sentence which made the optimization problem easier.\\n1\\nIntroduction\\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\\ncellent performance on difﬁcult problems such as speech recognition [13, 7] and visual object recog-\\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough',\n",
       "  'metadata': {'creationDate': \"D:20141215203257-05'00'\",\n",
       "   'title': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014',\n",
       "   'moddate': '2014-12-15T20:32:57-05:00',\n",
       "   'file_type': 'pdf',\n",
       "   'subject': '',\n",
       "   'source': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'doc_index': 2,\n",
       "   'page': 0,\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'producer': 'GPL Ghostscript GIT PRERELEASE 9.08',\n",
       "   'modDate': \"D:20141215203257-05'00'\",\n",
       "   'total_pages': 9,\n",
       "   'creationdate': '2014-12-15T20:32:57-05:00',\n",
       "   'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software',\n",
       "   'format': 'PDF 1.4',\n",
       "   'content_length': 919,\n",
       "   'file_path': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'author': '',\n",
       "   'source_file': 'encoder-decoder.pdf'},\n",
       "  'similarity_score': 0.30348801612854004,\n",
       "  'distance': 0.69651198387146,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_359fca11_2',\n",
       "  'content': 'sentences (but not target sentences) improved the LSTM’s performance markedly,\\nbecause doing so introduced many short term dependencies between the source\\nand the target sentence which made the optimization problem easier.\\n1\\nIntroduction\\nDeep Neural Networks (DNNs) are extremely powerful machine learning models that achieve ex-\\ncellent performance on difﬁcult problems such as speech recognition [13, 7] and visual object recog-\\nnition [19, 6, 21, 20]. DNNs are powerful because they can perform arbitrary parallel computation\\nfor a modest number of steps. A surprising example of the power of DNNs is their ability to sort\\nN N-bit numbers using only 2 hidden layers of quadratic size [27]. So, while neural networks are\\nrelated to conventional statistical models, they learn an intricate computation. Furthermore, large\\nDNNs can be trained with supervised backpropagation whenever the labeled training set has enough',\n",
       "  'metadata': {'content_length': 919,\n",
       "   'author': '',\n",
       "   'file_type': 'pdf',\n",
       "   'creationDate': \"D:20141215203257-05'00'\",\n",
       "   'source_file': 'encoder-decoder.pdf',\n",
       "   'doc_index': 2,\n",
       "   'producer': 'GPL Ghostscript GIT PRERELEASE 9.08',\n",
       "   'total_pages': 9,\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'modDate': \"D:20141215203257-05'00'\",\n",
       "   'file_path': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'title': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014',\n",
       "   'moddate': '2014-12-15T20:32:57-05:00',\n",
       "   'page': 0,\n",
       "   'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software',\n",
       "   'format': 'PDF 1.4',\n",
       "   'creationdate': '2014-12-15T20:32:57-05:00',\n",
       "   'source': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'subject': ''},\n",
       "  'similarity_score': 0.30348801612854004,\n",
       "  'distance': 0.69651198387146,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_c901233e_4',\n",
       "  'content': 'sequence of words representing the answer. It is therefore clear that a domain-independent method\\nthat learns to map sequences to sequences would be useful.\\nSequences pose a challenge for DNNs because they require that the dimensionality of the inputs and\\noutputs is known and ﬁxed. In this paper, we show that a straightforward application of the Long\\nShort-Term Memory (LSTM) architecture [16] can solve general sequence to sequence problems.\\nThe idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large ﬁxed-\\ndimensional vector representation, and then to use another LSTM to extract the output sequence\\nfrom that vector (ﬁg. 1). The second LSTM is essentially a recurrent neural network language model\\n[28, 23, 30] except that it is conditioned on the input sequence. The LSTM’s ability to successfully\\nlearn on data with long range temporal dependencies makes it a natural choice for this application',\n",
       "  'metadata': {'trapped': '',\n",
       "   'total_pages': 9,\n",
       "   'content_length': 940,\n",
       "   'subject': '',\n",
       "   'producer': 'GPL Ghostscript GIT PRERELEASE 9.08',\n",
       "   'title': 'arXiv:1409.3215v3  [cs.CL]  14 Dec 2014',\n",
       "   'creator': 'dvips(k) 5.991 Copyright 2011 Radical Eye Software',\n",
       "   'author': '',\n",
       "   'file_type': 'pdf',\n",
       "   'moddate': '2014-12-15T20:32:57-05:00',\n",
       "   'doc_index': 4,\n",
       "   'format': 'PDF 1.4',\n",
       "   'creationDate': \"D:20141215203257-05'00'\",\n",
       "   'creationdate': '2014-12-15T20:32:57-05:00',\n",
       "   'modDate': \"D:20141215203257-05'00'\",\n",
       "   'source': '..\\\\pdfs\\\\encoder-decoder.pdf',\n",
       "   'page': 1,\n",
       "   'keywords': '',\n",
       "   'source_file': 'encoder-decoder.pdf',\n",
       "   'file_path': '..\\\\pdfs\\\\encoder-decoder.pdf'},\n",
       "  'similarity_score': 0.27582478523254395,\n",
       "  'distance': 0.724175214767456,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\" Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901eff1c",
   "metadata": {},
   "source": [
    "## Integrating Vector Database with LLM Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ace741",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple RAG pipeline with groq llm\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(api_key=groq_api_key, model='llama-3.3-70b-versatile', temperature=0.1, max_tokens=1024)\n",
    "\n",
    "#simple rag function to retrieve doc and generate output\n",
    "def simple_rag(query, rag_retriever,llm,top_k=3):\n",
    "    #retrieve context\n",
    "    result = rag_retriever.retrieve(query=query,top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in result]) if result else \"\"\n",
    "    if not context:\n",
    "        return \"No relevent context found to answer the question.\"\n",
    "    \n",
    "    #generate the answer using groq llm\n",
    "    prompt = f\"\"\"use the following context to answer the question concisely\n",
    "        context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "    \n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt.format(context=context,query=query))\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf48e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: what is encoder-decoder architecture?\n",
      "Top_k: 3, Score_threshold: 0.0\n",
      "Genetating embedding for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 64.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents afer filter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The encoder-decoder architecture refers to the transformer architecture, which consists of two main modules: \n",
      "\n",
      "1. **Encoder Module**: Comprises Feed-Forward Layer, Multi-Head Attention Layer, Residual connections, and Add and Norm layers. It receives embedding input and generates parameter matrices (Q, K, V) for the Multi-Head Attention layer.\n",
      "2. **Decoder Module**: Similar to the Encoder module, but with additional layers such as Masked Multi-Head Attention. It also includes Feed-Forward, Multi-Head Attention, Residual connection, and Add layers.\n"
     ]
    }
   ],
   "source": [
    "answer = simple_rag(\"what is encoder-decoder architecture?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
