import os
from pathlib import Path
from dotenv import load_dotenv
from src.vectorstore import FaissVectorStore
from langchain_groq import ChatGroq

load_dotenv()

class RAGSearch:
    def __init__(self, persist_dir:str="faiss_store", model_name:str='all-MiniLM-L6-v2', llm_model='llama-3.3-70b-versatile'):
        self.vectorstore = FaissVectorStore(persist_dir,model_name)
        #Load or build model
        faiss_path = os.path.join(persist_dir,"faiss.index")
        meta_path = os.path.join(persist_dir,"metadata.pkl")
        if not (os.path.exists(faiss_path) and os.path.exists(meta_path)):
            from data_loader import load_all_documents
            docs = load_all_documents("data")
            self.vectorstore.build_from_documents(docs)
        else:
            self.vectorstore.load()
        groq_api_key = os.getenv("GROQ_API_KEY")
        self.llm = ChatGroq(api_key=groq_api_key,model_name=llm_model)
        print(f"[INFO] Groq LLM initialized: {llm_model}")

    def search_and_summarize(self, query:str, top_k:int=5)->str:
        results = self.vectorstore.query(query_text=query,top_k=top_k)
        text = [r["metadata"].get("text","") for r in results if r["metadata"]]
        context = '\n\n'.join(text)
        if not context:
            return "No relevent documents found."
        prompt = f"""Summarize the following context for the query: {query} \n\nContext: {context}"""
        response = self.llm.invoke(prompt)
        return response.content